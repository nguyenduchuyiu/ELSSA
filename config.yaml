# Audio Settings
sample_rate: 16000
input_device: null   # null = default input device, or specify device ID/name
output_device: null 

# Wake Word Detection
wake_word_model: "models/openwakeword/alexa_v0.1.tflite"
interrupt_wake_word_model: "models/openwakeword/alexa_v0.1.tflite"

# Session Management
silence_timeout: 2  
max_silence_retries: 3

# ASR Settings
asr_model: "tiny.en"
asr_timeout: 30
chunk_duration: 5.0
overlap_duration: 3.0
# meaningless_phrases: ["uh", "um", "ah", "eh", "mm", "hmm", "hm", "thank you", "thanks", "bye", "goodbye"]

# TTS Settings 
tts_engine: "coqui" # "openvoice"

## CoquiTTS
coqui_tts_model_path: "models/CoquiTTS/tts_models--en--ljspeech--tacotron2-DDC/model_file.pth"
coqui_tts_model_config_path: "models/CoquiTTS/tts_models--en--ljspeech--tacotron2-DDC/config.json"
coqui_tts_vocoder_model_path: "models/CoquiTTS/vocoder_models--en--ljspeech--hifigan_v2/model_file.pth"
coqui_tts_vocoder_model_config_path: "models/CoquiTTS/vocoder_models--en--ljspeech--hifigan_v2/config.json"
coqui_tts_gpu: False

## OpenVoiceTTS
openvoice_tts_ckpt_base_dir: "models/openvoice/checkpoints/base_speakers/EN"
openvoice_tts_ckpt_converter_dir: "models/openvoice/checkpoints/converter"
openvoice_tts_reference_speaker_path: "assets/audio/ref_voice.mp3"
openvoice_tts_output_dir: "models/openvoice/outputs"

tts_buffer_duration: 20
tts_timeout_buffer_wait: 30

# LLM Settings
# model_path: "models/xLAM/Salesforce.xLAM-2-3b-fc-r.Q4_K_M.gguf"
# model_path: "models/Phi/Phi-3-mini-128k-instruct.Q4_K_M.gguf"
# model_path: "models/Llama-3.2-3B-Instruct/Llama-3.2-3B-Instruct-Q6_K_L.gguf"
# model_path: "models/Llama-3.2-3B-Instruct/Llama-3.2-3B-Instruct-IQ3_M.gguf"
model_path: "models/TinyLlama/tinyllama-1.1b-chat-v1.0.Q8_0.gguf"
n_ctx: 16384
temperature: 0.7
max_tokens: 256
stop: ["<|user|>", "<|assistant|>", "< Mayer|>", "<3", "<_assistant_|>", "<_user_|>", "<|im_start|>", "<|im_end|>"]
n_gpu_layers: 0
n_batch: 512

# Context Management
max_context_length: 10

super_mode: false

system_prompt: |
  You are ELSSA, an intelligent and emotionally-aware AI assistant. 
  You are a helpful assistant. Answer user questions and help with tasks.
  You should talk concise and to the point.
# system_prompt: |
#   You are ELSSA, an intelligent and emotionally-aware AI assistant. 
#   You interact naturally, with a friendly, witty, and warm tone. 
#   You understand context, respond with empathy, and initiate natural conversation like a thoughtful friend.
#   You should talk concisely and to the point.

#   Follow this output pattern:
#    1. thoughts: Think what to do next (use the tools or not?, just chat with user or call tools?)
#    2. response: Answer to user
#    3. tool_calls: tools (if needed) or empty array if you don't need to use tools
#    4. is_final_answer: True if you want to end the turn, False if you want to continue the turn.

#   NOTES: 
#     1. If you use a tool, you should set is_final_answer to False to continue the turn to see the result of tool call.
#     2. If you want to ask user for confirmation, you should set is_final_answer to True to end the turn.


example_conversation: |
   Example conversation:
   History: user: "Find and open song Photograph by Ed Sheeran on YouTube"
   Turn 1:
   1. thoughts: "I need to find a song on YouTube."
   2. response: "Ok wait a moment! Now I'm searching for the video for you."
   3. tool_calls: [{"name": "youtube_search", "arguments": {"query": "Photograph by Ed Sheeran"}}]
   4. is_final_answer: False (continue turn to see tool call result)  
   Turn 2:
   History: user: "Find and open song Photograph by Ed Sheeran on YouTube"
            assistant: "Ok wait a moment! Now I'm searching for the video for you."
            assistant: Tool youtube_search executed successfully with result: 
                      {"success": true, "message": "Found video: Photograph by Ed Sheeran", "data": {"url": "https://www.youtube.com/watch?v=7NgCf0TPgW8"}}
   1. thoughts: "I found a song on YouTube. I need to ask user for confirmation." 
   2. response: "I've found it! Do you want to open it?" 
   3. tool_calls: [] # no tool calls (need to ask user for confirmation)
   4. is_final_answer: True (end turn to ask user for confirmation)
   After user confirm:
   History: user: "Find and open song Photograph by Ed Sheeran on YouTube"
            assistant: "I've found it! Now opening the video for you."
            assistant: Tool youtube_search executed successfully with result: 
                      {"success": true, "message": "Found video: Photograph by Ed Sheeran", "data": {"url": "https://www.youtube.com/watch?v=7NgCf0TPgW8"}}
            user: "Yes, open it."
   1. thoughts: "User confirmed to open the video. I need to open the video and wait for result." 
   2. response: "Opening the video for you."
   3. tool_calls: [{"name": "open_link", "arguments": {"url": "https://www.youtube.com/watch?v=7NgCf0TPgW8"}}]
   4. is_final_answer: False (continue turn to see tool call result)
   After tool call:
   History: user: "Find and open song Photograph by Ed Sheeran on YouTube"
            assistant: "Ok wait a moment! Now I'm searching for the video for you."
            assistant: Tool youtube_search executed successfully with result: 
                      {"success": true, "message": "Found video: Photograph by Ed Sheeran", "data": {"url": "https://www.youtube.com/watch?v=7NgCf0TPgW8"}}
            user: "Yes, open it."
            assistant: "Ok wait a moment! Now I'm opening the video for you."
            assistant: Tool open_link executed successfully with result: 
                      {"success": true, "message": "Opened link: https://www.youtube.com/watch?v=7NgCf0TPgW8"}
   1. thoughts: "User confirmed to open the video. I have opened the video. I need to confirm to user." 
   2. response: "Done! The video is now playing."
   3. tool_calls: [] # no tool calls (need to confirm to user)
   4. is_final_answer: True (your work is done, end turn)
